{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a35b8f-a889-40c0-b5d7-ada9f440d85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.11 (main, Dec 11 2024, 10:25:04) [Clang 14.0.6 ]\n",
      "2.5.1\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import torch\n",
    "\n",
    "# 当前安装的 PyTorch 库的版本\n",
    "print(torch.__version__)\n",
    "# 检查 CUDA 是否可用，即你的系统有 NVIDIA 的 GPU\n",
    "print(torch.backends.mps.is_available()) #检查mps是否可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddef1490-3711-4ce3-bc8a-4b268c3f1957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量 a:\n",
      "tensor([[ 0.9061,  0.5764, -0.1556],\n",
      "        [ 0.5135, -0.8620, -0.2591]])\n",
      "张量 b:\n",
      "tensor([[ 0.4754, -0.2096, -0.3188],\n",
      "        [-1.2194,  1.8171, -0.2972]])\n",
      "a 和 b 的逐元素乘积:\n",
      "tensor([[ 0.4307, -0.1208,  0.0496],\n",
      "        [-0.6262, -1.5663,  0.0770]])\n",
      "张量 a 所有元素的总和:\n",
      "tensor(0.7193)\n",
      "张量 a 第 2 行第 3 列的元素:\n",
      "tensor(-0.2591)\n",
      "张量 a 中的最大值:\n",
      "tensor(0.9061)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 设置数据类型和设备\n",
    "dtype = torch.float  # 张量数据类型为浮点型\n",
    "device = torch.device(\"cpu\")  # 本次计算在 CPU 上进行\n",
    "\n",
    "# 创建并打印两个随机张量 a 和 b\n",
    "a = torch.randn(2, 3, device=device, dtype=dtype)  # 创建一个 2x3 的随机张量\n",
    "b = torch.randn(2, 3, device=device, dtype=dtype)  # 创建另一个 2x3 的随机张量\n",
    "\n",
    "print(\"张量 a:\")\n",
    "print(a)\n",
    "\n",
    "print(\"张量 b:\")\n",
    "print(b)\n",
    "\n",
    "# 逐元素相乘并输出结果\n",
    "print(\"a 和 b 的逐元素乘积:\")\n",
    "print(a * b)\n",
    "\n",
    "# 输出张量 a 所有元素的总和\n",
    "print(\"张量 a 所有元素的总和:\")\n",
    "print(a.sum())\n",
    "\n",
    "# 输出张量 a 中第 2 行第 3 列的元素（注意索引从 0 开始）\n",
    "print(\"张量 a 第 2 行第 3 列的元素:\")\n",
    "print(a[1, 2])\n",
    "\n",
    "# 输出张量 a 中的最大值\n",
    "print(\"张量 a 中的最大值:\")\n",
    "print(a.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0de10752-36e6-4af8-bd6d-a3a9355c2f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[ 0.7647,  1.5718, -0.8094],\n",
      "        [-1.7747,  1.8050, -0.4163]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[ 1.1831,  0.2738, -0.6543],\n",
      "        [ 1.0931,  1.3279,  0.0479]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "张量（Tensor）\n",
    "张量（Tensor）是 PyTorch 中的核心数据结构，用于存储和操作多维数组。\n",
    "\n",
    "张量可以视为一个多维数组，支持加速计算的操作。\n",
    "\n",
    "在 PyTorch 中，张量的概念类似于 NumPy 中的数组，但是 PyTorch 的张量可以运行在不同的设备上，比如 CPU 和 GPU，这使得它们非常适合于进行大规模并行计算，特别是在深度学习领域。\n",
    "\n",
    "维度（Dimensionality）：张量的维度指的是数据的多维数组结构。例如，一个标量（0维张量）是一个单独的数字，一个向量（1维张量）是一个一维数组，一个矩阵（2维张量）是一个二维数组，以此类推。\n",
    "\n",
    "形状（Shape）：张量的形状是指每个维度上的大小。例如，一个形状为(3, 4)的张量意味着它有3行4列。\n",
    "\n",
    "数据类型（Dtype）：张量中的数据类型定义了存储每个元素所需的内存大小和解释方式。PyTorch支持多种数据类型，包括整数型（如torch.int8、torch.int32）、浮点型（如torch.float32、torch.float64）和布尔型（torch.bool）。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "张量创建：\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# 创建一个 2x3 的全 0 张量\n",
    "a = torch.zeros(2, 3)\n",
    "print(a)\n",
    "\n",
    "# 创建一个 2x3 的全 1 张量\n",
    "b = torch.ones(2, 3)\n",
    "print(b)\n",
    "\n",
    "# 创建一个 2x3 的随机数张量\n",
    "c = torch.randn(2, 3)\n",
    "print(c)\n",
    "\n",
    "# 从 NumPy 数组创建张量\n",
    "import numpy as np\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(tensor_from_numpy)\n",
    "\n",
    "# 在指定设备（CPU/GPU）上创建张量\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "d = torch.randn(2, 3, device=device)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86ca0e11-1f5a-4046-9c22-a85b0ba4e7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e is: \n",
      "tensor([[ 1.5503, -0.7458, -1.3833],\n",
      "        [ 1.0810, -1.0621, -0.2452]])\n",
      "f is: \n",
      "tensor([[ 0.2556, -0.1488, -1.0672],\n",
      "        [-0.3599,  0.8292, -0.4449]])\n",
      "e + f is:\n",
      "tensor([[ 1.8059, -0.8945, -2.4505],\n",
      "        [ 0.7211, -0.2329, -0.6901]])\n",
      "e * f is:\n",
      "tensor([[ 0.3962,  0.1109,  1.4762],\n",
      "        [-0.3890, -0.8807,  0.1091]])\n",
      "g.t() is:\n",
      "tensor([[ 1.3474, -0.6870, -0.0390],\n",
      "        [ 0.2927, -1.1792, -0.6246]])\n",
      "g.shape is:\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "常用张量操作：\n",
    "\"\"\"\n",
    "# 张量相加\n",
    "e = torch.randn(2, 3)\n",
    "print(f'e is: \\n{e}')\n",
    "f = torch.randn(2, 3)\n",
    "print(f'f is: \\n{f}')\n",
    "\n",
    "print(f'e + f is:\\n{e + f}')\n",
    "\n",
    "# 逐元素乘法\n",
    "print(f'e * f is:\\n{e * f}')\n",
    "\n",
    "# 张量的转置\n",
    "g = torch.randn(3, 2)\n",
    "print(f'g.t() is:\\n{g.t()}')  # 或者 g.transpose(0, 1)\n",
    "\n",
    "# 张量的形状\n",
    "print(f'g.shape is:\\n{g.shape}')  # 返回形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e993299-cbac-4cdb-8179-81c8da0341ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_cpu:\n",
      "tensor([[ 0.2171, -0.3179, -0.8427],\n",
      "        [-0.0697, -0.4948,  0.5928]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "张量与设备\n",
    "PyTorch 张量可以存在于不同的设备上，包括CPU和GPU，你可以将张量移动到 GPU 上以加速计算：\n",
    "\"\"\"\n",
    "tensor_from_list = torch.randn(2, 3)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor_gpu = tensor_from_list.to('cuda')  # 将张量移动到GPU\n",
    "    print(f'tensor_gpu:\\n{tensor_gpu}')\n",
    "\n",
    "else:\n",
    "    tensor_cpu = tensor_from_list.to('cpu')  # 将张量移动到CPU\n",
    "    print(f'tensor_cpu:\\n{tensor_cpu}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4019eba3-f428-462c-a9fc-09c4851daa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([1., 4., 9.], grad_fn=<PowBackward0>)\n",
      "tensor(14., grad_fn=<SumBackward0>)\n",
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "梯度和自动微分\n",
    "PyTorch的张量支持自动微分，这是深度学习中的关键特性。当你创建一个需要梯度的张量时，PyTorch可以自动计算其梯度：\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "在这个例子中，x.grad将包含每个元素平方的梯度，即2倍的元素值。这是因为对于函数f(x) = x^2，其导数f'(x) = 2x。\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# 创建一个需要计算梯度的张量\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(x)\n",
    "# 假设一个简单的函数 y = x^2\n",
    "y = x**2\n",
    "print(y)\n",
    "# 计算损失，这里简单使用y作为损失\n",
    "loss = y.sum()\n",
    "print(loss)\n",
    "# 反向传播计算梯度\n",
    "loss.backward()\n",
    "\n",
    "# 输出梯度\n",
    "print(x.grad)  # 应该输出 [2.0, 4.0, 6.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25856714-9de5-4d33-b0ff-b1254c9f64bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 27.,  48.],\n",
      "        [ 75., 108.]], grad_fn=<MulBackward0>)\n",
      "tensor(64.5000, grad_fn=<MeanBackward0>)\n",
      "tensor([[4.5000, 6.0000],\n",
      "        [7.5000, 9.0000]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "自动求导（Autograd）\n",
    "自动求导（Automatic Differentiation，简称Autograd）是深度学习框架中的一个核心特性，它允许计算机自动计算数学函数的导数。\n",
    "\n",
    "在深度学习中，自动求导主要用于两个方面：一是在训练神经网络时计算梯度，二是进行反向传播算法的实现。\n",
    "\n",
    "自动求导基于链式法则（Chain Rule），这是一个用于计算复杂函数导数的数学法则。链式法则表明，复合函数的导数是其各个组成部分导数的乘积。在深度学习中，模型通常是由许多层组成的复杂函数，自动求导能够高效地计算这些层的梯度。\n",
    "\n",
    "动态图与静态图：\n",
    "\n",
    "动态图（Dynamic Graph）：在动态图中，计算图在运行时动态构建。每次执行操作时，计算图都会更新，这使得调试和修改模型变得更加容易。PyTorch使用的是动态图。\n",
    "\n",
    "静态图（Static Graph）：在静态图中，计算图在开始执行之前构建完成，并且不会改变。TensorFlow最初使用的是静态图，但后来也支持动态图。\n",
    "\n",
    "PyTorch 提供了自动求导功能，通过 autograd 模块来自动计算梯度。\n",
    "\n",
    "torch.Tensor 对象有一个 requires_grad 属性，用于指示是否需要计算该张量的梯度。\n",
    "\n",
    "当你创建一个 requires_grad=True 的张量时，PyTorch 会自动跟踪所有对它的操作，以便在之后计算梯度。\n",
    "\n",
    "创建需要梯度的张量:\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# 创建一个需要计算梯度的张量\n",
    "# x = torch.randn(2, 2, requires_grad=True)\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "\n",
    "print(x)\n",
    "\n",
    "# 执行某些操作\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "out = z.mean()\n",
    "print(out)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "反向传播（Backpropagation）\n",
    "一旦定义了计算图，可以通过 .backward() 方法来计算梯度。\n",
    "\"\"\"\n",
    "# 反向传播，计算梯度\n",
    "out.backward()\n",
    "\n",
    "# 查看 x 的梯度\n",
    "print(x.grad)\n",
    "\n",
    "\"\"\"\n",
    "在神经网络训练中，自动求导主要用于实现反向传播算法。\n",
    "\n",
    "反向传播是一种通过计算损失函数关于网络参数的梯度来训练神经网络的方法。在每次迭代中，网络的前向传播会计算输出和损失，然后反向传播会计算损失关于每个参数的梯度，并使用这些梯度来更新参数。\n",
    "\n",
    "停止梯度计算\n",
    "如果你不希望某些张量的梯度被计算（例如，当你不需要反向传播时），可以使用 torch.no_grad() 或设置 requires_grad=False。\n",
    "\"\"\"\n",
    "# 使用 torch.no_grad() 禁用梯度计算\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e49d911-eba5-418a-9d11-2eb34b8d1d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "神经网络（nn.Module）\n",
    "神经网络是一种模仿人脑神经元连接的计算模型，由多层节点（神经元）组成，用于学习数据之间的复杂模式和关系。\n",
    "\n",
    "神经网络通过调整神经元之间的连接权重来优化预测结果，这一过程涉及前向传播、损失计算、反向传播和参数更新。\n",
    "\n",
    "神经网络的类型包括前馈神经网络、卷积神经网络（CNN）、循环神经网络（RNN）和长短期记忆网络（LSTM），它们在图像识别、语音处理、自然语言处理等多个领域都有广泛应用。\n",
    "\n",
    "PyTorch 提供了一个非常方便的接口来构建神经网络模型，即 torch.nn.Module。\n",
    "\n",
    "我们可以继承 nn.Module 类并定义自己的网络层。\n",
    "\n",
    "创建一个简单的神经网络：\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义一个简单的全连接神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层\n",
    "        self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU 激活函数\n",
    "        print(f'输入层到隐藏层:{x}')\n",
    "        x = self.fc2(x)\n",
    "        print(f'隐藏层到输出层:{x}')\n",
    "        return x\n",
    "\n",
    "# 创建网络实例\n",
    "model = SimpleNN()\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "174c8978-9e36-4d37-9637-98ce79e2eee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1122, -1.8023]])\n",
      "输入层到隐藏层:tensor([[0.0000, 1.5251]], grad_fn=<ReluBackward0>)\n",
      "隐藏层到输出层:tensor([[1.1807]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.1807]], grad_fn=<AddmmBackward0>)\n",
      "tensor(5.6985, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "训练过程：\n",
    "\n",
    "前向传播（Forward Propagation）： 在前向传播阶段，输入数据通过网络层传递，每层应用权重和激活函数，直到产生输出。\n",
    "\n",
    "计算损失（Calculate Loss）： 根据网络的输出和真实标签，计算损失函数的值。\n",
    "\n",
    "反向传播（Backpropagation）： 反向传播利用自动求导技术计算损失函数关于每个参数的梯度。\n",
    "\n",
    "参数更新（Parameter Update）： 使用优化器根据梯度更新网络的权重和偏置。\n",
    "\n",
    "迭代（Iteration）： 重复上述过程，直到模型在训练数据上的性能达到满意的水平。\n",
    "\n",
    "前向传播与损失计算\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# 随机输入\n",
    "x = torch.randn(1, 2)\n",
    "print(x)\n",
    "\n",
    "# 前向传播\n",
    "output = model(x)\n",
    "print(output)\n",
    "\n",
    "# 定义损失函数（例如均方误差 MSE）\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 假设目标值为 1\n",
    "target = torch.randn(1, 1)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f33803b-c830-4b84-ae16-024ce0699ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "优化器（Optimizers）\n",
    "优化器在训练过程中更新神经网络的参数，以减少损失函数的值。\n",
    "\n",
    "PyTorch 提供了多种优化器，例如 SGD、Adam 等。\n",
    "\n",
    "使用优化器进行参数更新：\n",
    "\"\"\"\n",
    "# 定义优化器（使用 Adam 优化器）\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练步骤\n",
    "optimizer.zero_grad()  # 清空梯度\n",
    "loss.backward()  # 反向传播\n",
    "optimizer.step()  # 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "054784d6-a747-4642-b149-3d9e72aced09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.2488\n",
      "Epoch [20/100], Loss: 1.2338\n",
      "Epoch [30/100], Loss: 1.2201\n",
      "Epoch [40/100], Loss: 1.2082\n",
      "Epoch [50/100], Loss: 1.1976\n",
      "Epoch [60/100], Loss: 1.1882\n",
      "Epoch [70/100], Loss: 1.1800\n",
      "Epoch [80/100], Loss: 1.1728\n",
      "Epoch [90/100], Loss: 1.1666\n",
      "Epoch [100/100], Loss: 1.1612\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "训练模型\n",
    "训练模型是机器学习和深度学习中的核心过程，旨在通过大量数据学习模型参数，以便模型能够对新的、未见过的数据做出准确的预测。\n",
    "\n",
    "训练模型通常包括以下几个步骤：\n",
    "\n",
    "数据准备：\n",
    "收集和处理数据，包括清洗、标准化和归一化。\n",
    "将数据分为训练集、验证集和测试集。\n",
    "\n",
    "定义模型：\n",
    "选择模型架构，例如决策树、神经网络等。\n",
    "初始化模型参数（权重和偏置）。\n",
    "\n",
    "选择损失函数：\n",
    "根据任务类型（如分类、回归）选择合适的损失函数。\n",
    "\n",
    "选择优化器：\n",
    "选择一个优化算法，如SGD、Adam等，来更新模型参数。\n",
    "\n",
    "前向传播：\n",
    "在每次迭代中，将输入数据通过模型传递，计算预测输出。\n",
    "\n",
    "计算损失：\n",
    "使用损失函数评估预测输出与真实标签之间的差异。\n",
    "\n",
    "反向传播：\n",
    "利用自动求导计算损失相对于模型参数的梯度。\n",
    "\n",
    "参数更新：\n",
    "根据计算出的梯度和优化器的策略更新模型参数。\n",
    "\n",
    "迭代优化：\n",
    "重复步骤5-8，直到模型在验证集上的性能不再提升或达到预定的迭代次数。\n",
    "\n",
    "评估和测试：\n",
    "使用测试集评估模型的最终性能，确保模型没有过拟合。\n",
    "\n",
    "模型调优：\n",
    "根据模型在测试集上的表现进行调参，如改变学习率、增加正则化等。\n",
    "\n",
    "部署模型：\n",
    "将训练好的模型部署到生产环境中，用于实际的预测任务。\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 定义一个简单的神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层\n",
    "        self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU 激活函数\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 2. 创建模型实例\n",
    "model = SimpleNN()\n",
    "\n",
    "# 3. 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()  # 均方误差损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 优化器\n",
    "\n",
    "# 4. 假设我们有训练数据 X 和 Y\n",
    "X = torch.randn(10, 2)  # 10 个样本，2 个特征\n",
    "Y = torch.randn(10, 1)  # 10 个目标值\n",
    "\n",
    "# 5. 训练循环\n",
    "for epoch in range(100):  # 训练 100 轮\n",
    "    optimizer.zero_grad()  # 清空之前的梯度\n",
    "    output = model(X)  # 前向传播\n",
    "    loss = criterion(output, Y)  # 计算损失\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新参数\n",
    "    \n",
    "    # 每 10 轮输出一次损失\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "\n",
    "\"\"\"\n",
    "在每 10 轮，程序会输出当前的损失值，帮助我们跟踪模型的训练进度。随着训练的进行，损失值应该会逐渐降低，表示模型在不断学习并优化其参数。\n",
    "\n",
    "训练模型是一个迭代的过程，需要不断地调整和优化，直到达到满意的性能。这个过程涉及到大量的实验和调优，目的是使模型在新的、未见过的数据上也能有良好的泛化能力。\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
