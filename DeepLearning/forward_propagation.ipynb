{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83013f1-f122-4bfe-8ddd-69cc26f7c080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:52:09) \n",
      "[Clang 14.0.6 ]\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ee3163-9959-48c0-998f-178187eba94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape： (60000, 28, 28) (60000,) \n",
      "dtype： <dtype: 'float32'> <dtype: 'int32'>\n",
      "x的最小值： tf.Tensor(0.0, shape=(), dtype=float32) \n",
      "x的最大值： tf.Tensor(255.0, shape=(), dtype=float32)\n",
      "y的最小值： tf.Tensor(0, shape=(), dtype=int32) \n",
      "y的最大值： tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. 数据获取\n",
    "首先，我们导入需要用到的库文件和数据集。导入的x和y数据是数组类型，需要转换成tensor类型tf.convert_to_tensor()，再查看一下我们读入的数据有没有问题。\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets # 数据集工具\n",
    "import os  # 设置一下输出框打印的内容\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # '2'输出栏只打印error信息，其他乱七八糟的信息不打印\n",
    " \n",
    "#（1）获取mnist数据集\n",
    "(x,y),_ = datasets.mnist.load_data() \n",
    "#（2）转换成tensor类型，x数据类型一般为float32，y存放的是图片属于哪种具体类型，属于整型\n",
    "x = tf.convert_to_tensor(x,dtype=tf.float32)\n",
    "y = tf.convert_to_tensor(y,dtype=tf.int32)\n",
    "#（3）查看数据内容\n",
    "print('shape：',x.shape,y.shape,'\\ndtype：',x.dtype,y.dtype)  #查看shape和数据类型\n",
    "print('x的最小值：',tf.reduce_min(x),'\\nx的最大值：',tf.reduce_max(x))  #查看x的数据范围\n",
    "print('y的最小值：',tf.reduce_min(y),'\\ny的最大值：',tf.reduce_max(y))  #查看y的数据范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fcff145-f5bb-4800-a7b3-038cd5d56a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: (128, 28, 28) (128, 10)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. 数据预处理\n",
    "首先对x数据进行归一化处理，原来x的每个像素值在[0,255]之间，现在转变成[0,1]之间。刚导入的y数据的shape是[6000]，\n",
    "一维，存放分类数，为了和最后的预测结果比较，对它one-hot编码，shape变为[6000,10]。存放每张图属于每一个分类的概率。\n",
    "y.numpy()[0]表示第0张图像属于第5个分类的概率是1，属于其他分类的概率是0。\n",
    "再设置一个学习率lr，用于每次迭代完成后更新神经网络权重参数，初始学习率以 0.01 ~ 0.001 为宜。\n",
    "\"\"\"\n",
    "#（4）预处理\n",
    "x = x/255.  # 归一化处理，将x数据的范围从[0,255]变成[0,1]\n",
    "y = tf.one_hot(y,depth=10) # y是分类数值，对它进行one-hot编码，shape变为[b,10]\n",
    "y.numpy()[0]  # 查看编码后的y的数据\n",
    "# array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)\n",
    " \n",
    "lr = 1e-3 # 学习率，设置过大，会导致loss震荡，学习难以收敛；设置过小，那么训练的过程将大大增加\n",
    "\n",
    "\"\"\"\n",
    "加载数据集tf.data.Dataset.from_tensor_slices()，生成迭代器的 iter()，返回迭代器的下一个项目next() \n",
    "\"\"\"\n",
    "#（5）指定一个选取数据的batch，一次取128个数据\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128)\n",
    "train_iter = iter(train_db) # 指定迭代器\n",
    "sample = next(train_iter) # 存放的每一个batch\n",
    "# sample[0]存放x数据，sample[1]存放y数据，每个sample有128组图片\n",
    "print('batch:',sample[0].shape,sample[1].shape)\n",
    "# 打印结果： batch: (128, 28, 28) (128, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f5c2f4-7b53-4794-8406-a94af582ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. 构建网络\n",
    "输入特征x的shape为[128,28,28]，即输入层有28*28个神经元，自定义隐含层1有256个神经元，隐含层2有128个神经元，最终输出结果是固定的10个分类。\n",
    "\n",
    "根据每一层神经元的个数来确定每个连接层的shape，使用随机的截断高斯分布来初始化各个权重参数，将定义的变量从tensor类型，转变为神经网络类型variable类型。\n",
    "\"\"\"\n",
    "#（6）构建网络\n",
    "# 输入层由输入多少个特征点决定，输出层根据有多少个分类决定\n",
    "# 输入层shape[b,784]，输出层shape[b,10]\n",
    "# 构建网络，自定义中间层的神经元个数\n",
    "# [b,784] => [b,256] => [b,128] => [b,10]\n",
    " \n",
    "# 第一个连接层的权重和偏置，都变成tf.Variable类型，这样tf.GradientTape才能记录梯度信息\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784,256], stddev=0.1)) # 截断正态分布，标准差调小一点防止梯度爆炸\n",
    "b1 = tf.Variable(tf.zeros([256])) #维度为[dim_out]\n",
    "# 第二个连接层的权重和偏置\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256,128], stddev=0.1)) # 截断正态分布，维度为[dim_in, dim_out]\n",
    "b2 = tf.Variable(tf.zeros([128])) #维度为[dim_out]\n",
    "# 第三个连接层的权重和偏置\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128,10], stddev=0.1)) # 截断正态分布，维度为[dim_in, dim_out]\n",
    "b3 = tf.Variable(tf.zeros([10])) #维度为[dim_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265f334e-09e6-42a7-9989-92b8cd75c11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次迭代，loss为0.5268236398696899\n",
      "第101次迭代，loss为0.20985133945941925\n",
      "第201次迭代，loss为0.1924140453338623\n",
      "第301次迭代，loss为0.17324678599834442\n",
      "第401次迭代，loss为0.1812964230775833\n",
      "第1次迭代，loss为0.16923733055591583\n",
      "第101次迭代，loss为0.15967503190040588\n",
      "第201次迭代，loss为0.1552380919456482\n",
      "第301次迭代，loss为0.14402899146080017\n",
      "第401次迭代，loss为0.15083137154579163\n",
      "第1次迭代，loss为0.14145183563232422\n",
      "第101次迭代，loss为0.13724394142627716\n",
      "第201次迭代，loss为0.1339339315891266\n",
      "第301次迭代，loss为0.12667378783226013\n",
      "第401次迭代，loss为0.1327119767665863\n",
      "第1次迭代，loss为0.12423168122768402\n",
      "第101次迭代，loss为0.12314923107624054\n",
      "第201次迭代，loss为0.12016479671001434\n",
      "第301次迭代，loss为0.11497267335653305\n",
      "第401次迭代，loss为0.12071685492992401\n",
      "第1次迭代，loss为0.1124691590666771\n",
      "第101次迭代，loss为0.11352002620697021\n",
      "第201次迭代，loss为0.11049656569957733\n",
      "第301次迭代，loss为0.10650961101055145\n",
      "第401次迭代，loss为0.11214251816272736\n",
      "第1次迭代，loss为0.10395033657550812\n",
      "第101次迭代，loss为0.10639822483062744\n",
      "第201次迭代，loss为0.10331796109676361\n",
      "第301次迭代，loss为0.10005316883325577\n",
      "第401次迭代，loss为0.10571111738681793\n",
      "第1次迭代，loss为0.09747450053691864\n",
      "第101次迭代，loss为0.10086596012115479\n",
      "第201次迭代，loss为0.0977364331483841\n",
      "第301次迭代，loss为0.0949610099196434\n",
      "第401次迭代，loss为0.10066692531108856\n",
      "第1次迭代，loss为0.09237078577280045\n",
      "第101次迭代，loss为0.09636745601892471\n",
      "第201次迭代，loss为0.09318109601736069\n",
      "第301次迭代，loss为0.09078700840473175\n",
      "第401次迭代，loss为0.0965443104505539\n",
      "第1次迭代，loss为0.08821485936641693\n",
      "第101次迭代，loss为0.09262319654226303\n",
      "第201次迭代，loss为0.08939202129840851\n",
      "第301次迭代，loss为0.08730694651603699\n",
      "第401次迭代，loss为0.0931052416563034\n",
      "第1次迭代，loss为0.0847066268324852\n",
      "第101次迭代，loss为0.08941274881362915\n",
      "第201次迭代，loss为0.08616068959236145\n",
      "第301次迭代，loss为0.08432866632938385\n",
      "第401次迭代，loss为0.09014055877923965\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4. 前向传播运算\n",
    "每一次迭代从train_db中取出128个样本数据，由于取出的x数据的shape是[128,28,28]，需要将它的形状转变成[128,28*28]才能传入输入层tf.reshape()。\n",
    "h = x @ w + b，本层的特征向量和权重做内积，再加上偏置，将计算结果放入激活函数tf.nn.relu()，得到下一层的输入特征向量。\n",
    "最终得到的输出结果out中存放的是每张图片属于每个分类的概率。\n",
    "\"\"\"\n",
    "#（7）前向传播运算\n",
    "for i in range(10):  #对整个数据集迭代10次\n",
    "    # 对数据集的所有batch迭代一次\n",
    "    # x为输入的特征项，shape为[128,28,28]，y为分类结果，shape为[128,10]\n",
    "    for step,(x,y) in enumerate(train_db): # 返回下标和对应的值\n",
    "        # 这里的x的shape为[b,28*28]，从[b,w,h]变成[b,w*h]\n",
    "        x = tf.reshape(x,[-1,28*28]) #对输入特征项的维度变换，-1会自动计算b\n",
    "    \n",
    "        with tf.GradientTape() as tape: # 自动求导计算梯度，只会跟踪tf.Variable类型数据\n",
    "            # ==1== 从输入层到隐含层1的计算方法为：h1 = w1 @ x1 + b1   \n",
    "            # [b,784] @ [784,256] + [b,256] = [b,256]\n",
    "            h1 = x @ w1 + b1  # 相加时会自动广播，改变b的shape，自动进行tf.broadcast_to(b1,[x.shape[0],256])\n",
    "            # 激活函数，relu函数\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            # ==2== 从隐含层1到隐含层2，[b,256] @ [256,128] + [b,128] = [b,128]\n",
    "            h2 = h1 @ w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            # ==3== 从隐含层2到输出层，[b,128] @ [128,10] + [b,10] = [b,10]\n",
    "            out = h2 @ w3 + b3 # shape为[b,10]\n",
    "            \n",
    "            #（8）计算误差，输出值out的shape为[b,10]，onehot编码后真实值y的shape为[b,10]\n",
    "            # 计算均方差 mse = mean(sum((y-out)^2)\n",
    "            loss_square = tf.square(y-out)  # shape为[b,10]\n",
    "            loss = tf.reduce_mean(loss_square) # 得到一个标量\n",
    "            \n",
    "        # 梯度计算\n",
    "        grads = tape.gradient(loss,[w1,b1,w2,b2,w3,b3])\n",
    "        \n",
    "        # 注意：下面的方法，运算返回值是tf.tensor类型，在下一次运算会出现错误\n",
    "        # w1 = w1 - lr * grads[0] # grads[0]值梯度计算返回的w1，是grad的第0个元素\n",
    " \n",
    "        # 权重更新，lr为学习率，梯度每次下降多少\n",
    "        # 因此需要原地更新函数，保证更新后的数据类型不变tf.Variable\n",
    "        w1.assign_sub(lr * grads[0])\n",
    "        b1.assign_sub(lr * grads[1])    \n",
    "        w2.assign_sub(lr * grads[2])  \n",
    "        b2.assign_sub(lr * grads[3]) \n",
    "        w3.assign_sub(lr * grads[4])    \n",
    "        b3.assign_sub(lr * grads[5]) \n",
    "        \n",
    "        if step % 100 == 0: #每100次显示一次数据\n",
    "            print(f\"第{step+1}次迭代，loss为{np.float64(loss)}\") #loss是tensor变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14b5ca2-50ff-46a6-933b-4c345ff03285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape： (60000, 28, 28) (60000,) \n",
      "dtype： <dtype: 'float32'> <dtype: 'int32'>\n",
      "x的最小值： tf.Tensor(0.0, shape=(), dtype=float32) \n",
      "x的最大值： tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "y的最小值： tf.Tensor(0, shape=(), dtype=int32) \n",
      "y的最大值： tf.Tensor(9, shape=(), dtype=int32)\n",
      "第1次迭代，loss为0.41158944368362427\n",
      "第101次迭代，loss为0.17804591357707977\n",
      "第201次迭代，loss为0.1776295006275177\n",
      "第301次迭代，loss为0.1620107740163803\n",
      "第401次迭代，loss为0.1635483205318451\n",
      "accuracy=0.1827\n",
      "第1次迭代，loss为0.15071257948875427\n",
      "第101次迭代，loss为0.13763673603534698\n",
      "第201次迭代，loss为0.14635491371154785\n",
      "第301次迭代，loss为0.13786068558692932\n",
      "第401次迭代，loss为0.13953611254692078\n",
      "accuracy=0.2488\n",
      "第1次迭代，loss为0.12772175669670105\n",
      "第101次迭代，loss为0.12205646932125092\n",
      "第201次迭代，loss为0.12839604914188385\n",
      "第301次迭代，loss为0.12246403843164444\n",
      "第401次迭代，loss为0.12415006011724472\n",
      "accuracy=0.3091\n",
      "第1次迭代，loss为0.11275205761194229\n",
      "第101次迭代，loss为0.11174468696117401\n",
      "第201次迭代，loss为0.11603144556283951\n",
      "第301次迭代，loss为0.11174885928630829\n",
      "第401次迭代，loss为0.11360490322113037\n",
      "accuracy=0.3576\n",
      "第1次迭代，loss为0.10220472514629364\n",
      "第101次迭代，loss为0.10432589054107666\n",
      "第201次迭代，loss为0.10690461099147797\n",
      "第301次迭代，loss为0.10376779735088348\n",
      "第401次迭代，loss为0.10591228306293488\n",
      "accuracy=0.403\n",
      "第1次迭代，loss为0.09438861906528473\n",
      "第101次迭代，loss为0.09861283749341965\n",
      "第201次迭代，loss为0.09990376979112625\n",
      "第301次迭代，loss为0.09755337238311768\n",
      "第401次迭代，loss为0.0999833345413208\n",
      "accuracy=0.4391\n",
      "第1次迭代，loss为0.08836541324853897\n",
      "第101次迭代，loss为0.09399668872356415\n",
      "第201次迭代，loss为0.09428030252456665\n",
      "第301次迭代，loss为0.09253354370594025\n",
      "第401次迭代，loss为0.09531505405902863\n",
      "accuracy=0.473\n",
      "第1次迭代，loss为0.08354724943637848\n",
      "第101次迭代，loss为0.09015579521656036\n",
      "第201次迭代，loss为0.08966526389122009\n",
      "第301次迭代，loss为0.08842542767524719\n",
      "第401次迭代，loss为0.09146930277347565\n",
      "accuracy=0.5029\n",
      "第1次迭代，loss为0.07958252727985382\n",
      "第101次迭代，loss为0.08691083639860153\n",
      "第201次迭代，loss为0.08581263571977615\n",
      "第301次迭代，loss为0.08497141301631927\n",
      "第401次迭代，loss为0.08820362389087677\n",
      "accuracy=0.5261\n",
      "第1次迭代，loss为0.07627274096012115\n",
      "第101次迭代，loss为0.08413735032081604\n",
      "第201次迭代，loss为0.08251472562551498\n",
      "第301次迭代，loss为0.08203335851430893\n",
      "第401次迭代，loss为0.08540233224630356\n",
      "accuracy=0.5449\n"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets # 数据集工具\n",
    "import os  # 设置一下输出框打印的内容\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # '2'输出栏只打印error信息，其他乱七八糟的信息不打印\n",
    " \n",
    "#（1）获取mnist数据集\n",
    "(x,y),(x_test,y_test) = datasets.mnist.load_data() \n",
    " \n",
    "#（2）转换成tensor类型，x数据类型一般为float32，y存放的是图片属于哪种具体类型，属于整型\n",
    "x = tf.convert_to_tensor(x,dtype=tf.float32)/255.0\n",
    "y = tf.convert_to_tensor(y,dtype=tf.int32)\n",
    " \n",
    "x_test = tf.convert_to_tensor(x_test,dtype=tf.float32)/255.0\n",
    "y_test = tf.convert_to_tensor(y_test,dtype=tf.int32)\n",
    " \n",
    "#（3）查看数据内容\n",
    "print('shape：',x.shape,y.shape,'\\ndtype：',x.dtype,y.dtype)  #查看shape和数据类型\n",
    "print('x的最小值：',tf.reduce_min(x),'\\nx的最大值：',tf.reduce_max(x))  #查看x的数据范围\n",
    "print('y的最小值：',tf.reduce_min(y),'\\ny的最大值：',tf.reduce_max(y))  #查看y的数据范围\n",
    " \n",
    "#（4）预处理\n",
    "y = tf.one_hot(y,depth=10) # 对训练数据的目标集的数值编码，变为长度为10的向量，对应索引的值变为1\n",
    "lr = 1e-3 # 指定学习率\n",
    " \n",
    "#（5）指定一个选取数据的batch，一次取128个数据\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(128)\n",
    " \n",
    "#（6）构建网络\n",
    "# 构建网络，自定义中间层的神经元个数\n",
    "# [b,784] => [b,256] => [b,128] => [b,10]\n",
    " \n",
    "# 第一个连接层的权重和偏置，都变成tf.Variable类型，这样tf.GradientTape才能记录梯度信息\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784,256], stddev=0.1)) # 截断正态分布，标准差调小一点防止梯度爆炸\n",
    "b1 = tf.Variable(tf.zeros([256])) #维度为[dim_out]\n",
    "# 第二个连接层的权重和偏置\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256,128], stddev=0.1)) # 截断正态分布，维度为[dim_in, dim_out]\n",
    "b2 = tf.Variable(tf.zeros([128])) #维度为[dim_out]\n",
    "# 第三个连接层的权重和偏置\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128,10], stddev=0.1)) # 截断正态分布，维度为[dim_in, dim_out]\n",
    "b3 = tf.Variable(tf.zeros([10])) #维度为[dim_out]\n",
    " \n",
    "#（7）前向传播运算\n",
    "for i in range(10):  #对整个数据集迭代10次\n",
    "    # 对数据集的所有batch迭代一次\n",
    "    # x为输入的特征项，shape为[128,28,28]，y为分类结果，shape为[128]\n",
    "    for step,(x,y) in enumerate(train_db): # 返回下标和对应的值\n",
    "        # 这里的x的shape为[b,28*28]，从[b,w,h]变成[b,w*h]\n",
    "        x = tf.reshape(x,[-1,28*28]) #对输入特征项的维度变换，-1会自动计算b\n",
    "    \n",
    "        with tf.GradientTape() as tape: # 自动求导计算梯度，只会跟踪tf.Variable类型数据\n",
    "            # ==1== 从输入层到隐含层1的计算方法为：h1 = w1 @ x1 + b1   \n",
    "            # [b,784] @ [784,256] + [b,256] = [b,256]\n",
    "            h1 = x @ w1 + b1  # 相加时会自动广播，改变b的shape，自动进行tf.broadcast_to(b1,[x.shape[0],256])\n",
    "            # 激活函数，relu函数\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            # ==2== 从隐含层1到隐含层2，[b,256] @ [256,128] + [b,128] = [b,128]\n",
    "            h2 = h1 @ w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            # ==3== 从隐含层2到输出层，[b,128] @ [128,10] + [b,10] = [b,10]\n",
    "            out = h2 @ w3 + b3 # shape为[b,10]\n",
    "            \n",
    "            #（8）计算误差，输出值out的shape为[b,10]，onehot编码后真实值y的shape为[b,10]\n",
    "            # 计算均方差 mse = mean(sum((y-out)^2)\n",
    "            loss_square = tf.square(y-out)  # shape为[b,10]\n",
    "            loss = tf.reduce_mean(loss_square) # 得到一个标量\n",
    "            \n",
    "        # 梯度计算\n",
    "        grads = tape.gradient(loss,[w1,b1,w2,b2,w3,b3])\n",
    "        # 权重更新，lr为学习率，梯度每次下降多少\n",
    "        \n",
    "        # 注意：下面的方法，运算返回值是tf.tensor类型，在下一次运算会出现错误\n",
    "        # w1 = w1 - lr * grads[0] # 返回的w1在grad的第0个元素\n",
    "        \n",
    "        # 因此需要原地更新函数，保证更新后的数据类型不变tf.Variable\n",
    "        w1.assign_sub(lr * grads[0])\n",
    "        b1.assign_sub(lr * grads[1])    \n",
    "        w2.assign_sub(lr * grads[2])  \n",
    "        b2.assign_sub(lr * grads[3]) \n",
    "        w3.assign_sub(lr * grads[4])    \n",
    "        b3.assign_sub(lr * grads[5]) \n",
    "        \n",
    "        if step % 100 == 0: #每100次显示一次数据\n",
    "            print(f\"第{step+1}次迭代，loss为{np.float64(loss)}\") #loss是tensor变量\n",
    "            # loss为nan，出现梯度爆炸的情况，初始化权重的时候变小一点\n",
    "    \n",
    "    #（9）网络测试\n",
    "    # total_correct统计预测对了的个数，total_num统计参与测试的个数\n",
    "    total_correct, total_num = 0, 0\n",
    "    # 对网络测试，test必须要使用当前阶段的权重w和偏置b，测试当前阶段的计算精度\n",
    "    for step,(x,y) in enumerate(test_db):\n",
    "        x = tf.reshape(x,[-1,28*28])\n",
    "        # 对测试数据进行前向传播\n",
    "        # 输入层[b,784] => [b,256] => [b,128] => 输出层[b,10]\n",
    "        h1 = x @ w1 + b1 # 输入特征和第一层权重做内积，结果加上偏置\n",
    "        h1 = tf.nn.relu(h1) # 计算后的结果放入激活函数中计算，得到下一层的输入\n",
    "        # 隐含层1=>隐含层2\n",
    "        h2 = tf.nn.relu(h1 @ w2 + b2)  # [b,256] => [b,128]\n",
    "        # 隐含层2=>输出层\n",
    "        out = h2 @ w3 + b3  # [b,128] => 输出层[b,10]\n",
    "        \n",
    "        #（10）输出概率计算\n",
    "        # 计算概率，out：shape为[b,10]，属于实数；prob：shape为[b,10]，属于[0,1]之间\n",
    "        # 使用softmax()函数，使实数out映射到[0,1]的范围\n",
    "        prob = tf.nn.softmax(out,axis=1) # out是二维的，在第1个轴上映射，即把某张图片属于某个类的值变成概率\n",
    "        # 概率最大的值所在的位置索引是预测结果，即属于第几个分类\n",
    "        predict = tf.argmax(prob, axis=1,output_type=tf.int32) # 每张图片的概率最大值所在位置，shape[b,10]，10所在的维度\n",
    "        # 测试集的y不需要转换成one-hot，真实值y是一个数值，predict也是一个数值，两两相比较\n",
    "        correct = tf.equal(predict, y) # 比较，返回布尔类型，相同为True，不同为False\n",
    "        correct = tf.cast(correct,dtype=tf.int32) # 将布尔类型转换为int32类型\n",
    "        correct = tf.reduce_sum(correct) # 每次取出的batch中，有多少张图片是预测对的\n",
    "        # 1代表预测对了，0代表预测错了\n",
    "        total_correct += int(correct)  # 统计对的个数，correct是tensor类型\n",
    "        total_num += x.shape[0]  # 统计一共有多少组参与测试，x.shape=[128,28*28]，x.shape[0]有128张图片\n",
    "        \n",
    "    # 数据集整体完成一次迭代后\n",
    "    accuracy = total_correct / total_num # 计算每一轮循环的准确率\n",
    "    print(f'accuracy={accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
