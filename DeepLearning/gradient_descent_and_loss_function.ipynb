{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af834e56-f195-42a9-8e69-0bb39cebe884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:52:09) \n",
      "[Clang 14.0.6 ]\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40654144-35e4-48a6-8365-4ac5afffff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=2.0>]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. 梯度下降\n",
    "1.1 自动求导函数：\n",
    "GradientTape(persistent=False, watch_accessed_variables=True)\n",
    "persistent： 布尔值，用来指定新创建的gradient tape是否是可持续性的。默认是False，意味着只能够调用一次GradientTape()函数。\n",
    "\n",
    "watch_accessed_variables： 布尔值，表明GradientTape()函数是否会自动追踪任何能被训练的变量。默认是True。要是为False的话，\n",
    "意味着你需要手动去指定你想追踪的那些变量。\n",
    "\n",
    "1.2 监视非Variable变量\n",
    "tape.watch(tensor)\n",
    "tape.watch()用于跟踪指定的tensor变量。由于GradientTape()默认只对tf.Variable类型的变量进行监控。如果需要监控的变量是tensor类型，则需要tape.watch()来监控，\n",
    "若是没有watch函数则梯度计算结果会是None。  如果指定跟踪的是tf.Variable类型，这一句就不用写了。\n",
    "\n",
    "1.3 梯度计算\n",
    "tape.gradient(target, sources, unconnected_gradients)\n",
    "根据指定监视的变量来计算梯度\n",
    "target： 求导的因变量\n",
    "sources： 求导的自变量\n",
    "unconnected_gradients： 无法求导时，返回的值，有两个可选值[none, zero]，默认none。\n",
    "\n",
    "创建的权重w是tensor类型，需要tape.watch()函数，如果改成w = tf.Variable(tf.constant(1.))，\n",
    "将w变成Variable类型，就不需要tape.watch()函数来监视w。y=x*w，y对w求导结果为x，即为2.0。\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "w = tf.constant(1.) # 创建全连接层\n",
    "print(w)\n",
    "x = tf.constant(2.) # 创建输入特征\n",
    "print(x)\n",
    "# 自动求导\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([w]) # 监视w\n",
    "    y = x*w \n",
    "# 计算梯度，因变量y，自变量w\n",
    "grad1 = tape.gradient(y,[w])\n",
    "# 结果为： [<tf.Tensor: shape=(), dtype=float32, numpy=2.0>]\n",
    "print(grad1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a7b3ee-1997-42c1-b818-4317b418a25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3 0 2], shape=(5,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]], shape=(5, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]], shape=(5, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.24972999  2.425936    0.00926983  1.4380516 ]\n",
      " [ 1.273383    0.09548399  0.2513505  -0.4188675 ]\n",
      " [-1.4276278   0.95170194  2.109021    0.81278783]\n",
      " [ 1.1616389  -0.34660465  1.1430593   1.0674406 ]\n",
      " [-1.1307237  -0.59858614 -1.4567995  -0.24784333]], shape=(5, 4), dtype=float32)\n",
      "tf.Tensor(1.2141784, shape=(), dtype=float32)\n",
      "tf.Tensor(1.2141783, shape=(), dtype=float32)\n",
      "tf.Tensor(1.2141783, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. 损失函数及其梯度\n",
    "2.1 均方误差MSE\n",
    "计算公式：loss =\\frac{1}{N} \\sum (y-pred)^{2}\n",
    "\n",
    "y代表训练集的真实值，pred代表训练输出的预测结果，N通常指的是batch_size，也有时候是指特征属性个数。\n",
    "\n",
    "MSE函数表示：\n",
    "(1) tf.reduce_mean(tf.square(y - pred))\n",
    "(2) tf.reduce_mean(tf.losses.MSE(y, pred))\n",
    "一般而言，均方误差损失函数比较适用于回归问题中，对于分类问题，特别是目标输出为One-hot向量的分类任务中，交叉熵损失函数要合适的多。\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "# 设有一组训练集的目标值\n",
    "y = tf.constant([1,2,3,0,2]) \n",
    "print(y)\n",
    "y = tf.one_hot(y, depth=4) # 目标为四种分类\n",
    "print(y)\n",
    "y = tf.cast(y,dtype=tf.float32)\n",
    "print(y)\n",
    "# 设有一组模型输出的预测结果数据\n",
    "out = tf.random.normal([5,4])\n",
    "print(out)\n",
    "# 三种方法计算，预测结果out和真实值y之间的损失MSE\n",
    "loss1 = tf.reduce_mean(tf.square(y-out))  # 1.2804947\n",
    "print(loss1)\n",
    "loss2 = tf.square(tf.norm(y-out))/(5*4) # 1.2804947  #二范数方法\n",
    "print(loss2)\n",
    "loss3 = tf.reduce_mean(tf.losses.MSE(y,out)) # 1.2804947\n",
    "print(loss3)\n",
    "# 返回shape为[b]的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec9d9d82-2845-4d99-9e84-8fcfb84c12e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.16336171 -1.1419387   1.050901    0.96142256]\n",
      " [-0.86587214 -0.16246136 -0.13545969  0.30557716]], shape=(2, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.03978245 -0.01595307 -1.6576735 ]\n",
      " [ 0.5839298  -1.4181936  -1.3043804 ]\n",
      " [-0.03014809  0.25292218 -0.22926952]\n",
      " [-0.71595144 -0.14785656 -0.47523195]], shape=(4, 3), dtype=float32)\n",
      "tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([2 0], shape=(2,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.03293834 0.7465724  0.22048923]\n",
      " [0.10872269 0.18083413 0.7104432 ]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(0.41631633, shape=(), dtype=float32)\n",
      "grads\n",
      " [<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[ 0.04115774,  0.02818924, -0.06934698],\n",
      "       [ 0.01227526, -0.09975903,  0.08748379],\n",
      "       [ 0.00245763,  0.09626692, -0.09872455],\n",
      "       [-0.01848195,  0.08134693, -0.06286498]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.05214864,  0.07393254, -0.0217839 ], dtype=float32)>]\n",
      "grads[0]\n",
      " tf.Tensor(\n",
      "[[ 0.04115774  0.02818924 -0.06934698]\n",
      " [ 0.01227526 -0.09975903  0.08748379]\n",
      " [ 0.00245763  0.09626692 -0.09872455]\n",
      " [-0.01848195  0.08134693 -0.06286498]], shape=(4, 3), dtype=float32)\n",
      "grads[1]\n",
      " tf.Tensor([-0.05214864  0.07393254 -0.0217839 ], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.2 MSE的梯度\n",
    "使用tf.reduce_mean(tf.losses.MSE())计算真实值和预测结果的均方差，需要对真实值y进行one-hot编码tf.one_hot()，对应索引位置的值为1，分成三个类别depth=3。\n",
    "prob输出的也是图片属于三种分类的概率。使用tape.gradient()函数对跟踪的变量求梯度，grads[0]因变量为loss，自变量为w。\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "#（1）均方差MSE\n",
    "x = tf.random.normal([2,4]) # 创建输入层，2张图片，各有4个特征\n",
    "print(x)\n",
    "w = tf.random.normal([4,3]) #一层全连接层，输出每张图片属于3个分类的结果\n",
    "print(w)\n",
    "b = tf.zeros([3]) # 三个偏置\n",
    "print(b)\n",
    "y = tf.constant([2,0]) # 真实值，第一个样本属于2类别，第2个样本属于0类别\n",
    "print(y)\n",
    "print(tf.one_hot(y,depth=3))\n",
    "#梯度计算\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([w,b]) # 指定观测w和b，如果w和b时variable类型，就不需要watch了\n",
    "    prob = tf.nn.softmax(x@w+b, axis=1) #将实数转为概率，得到属于3个节点的概率\n",
    "    print(prob)\n",
    "    # 计算两个样本损失函数的均方差\n",
    "    loss = tf.reduce_mean(tf.losses.MSE(tf.one_hot(y,depth=3),prob))\n",
    "    print(loss)\n",
    "# 求梯度\n",
    "grads = tape.gradient(loss,[w,b])\n",
    "print('grads\\n', grads)\n",
    "grads[0] # loss对w的梯度\n",
    "print('grads[0]\\n', grads[0])\n",
    "grads[1] # loss对b的梯度\n",
    "print('grads[1]\\n', grads[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39c28f71-5239-4953-a27a-963724711483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.3862944, shape=(), dtype=float32)\n",
      "tf.Tensor(2.3025851, shape=(), dtype=float32)\n",
      "tf.Tensor(0.35667497, shape=(), dtype=float32)\n",
      "=================================================================================================================================\n",
      "tf.Tensor(0.10536041, shape=(), dtype=float32)\n",
      "tf.Tensor(2.3025851, shape=(), dtype=float32)\n",
      "=================================================================================================================================\n",
      "tf.Tensor([[-7.852525 24.933382]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.3 交叉熵\n",
    "交叉熵（Cross Entropy）主要用于度量两个概率分布间的差异性信息，交叉熵越小，两者之间差异越小，当交叉熵等于0时达到最佳状态，也即是预测值与真实值完全吻合。\n",
    "公式为：H(p,q) = -\\sum p(x)log\\;q(x)\n",
    "式中，p(x)是真实分布的概率，q(x)是模型输出的预测概率。log的底数为2。\n",
    "\n",
    "(1) 多分类问题交叉熵\n",
    "tf.losses.categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
    "y_true： 真实值，需要one-hot编码\n",
    "y_pred： 预测值，模型输出结果\n",
    "from_logits： 为True时，会使用softmax函数将y_pred从实数转化为概率值，通常情况下用True结果更稳定\n",
    "\"\"\"\n",
    "# 多分类\n",
    "# 真实值和预测概率，预测结果均匀，交叉熵1.38很大\n",
    "print(tf.losses.categorical_crossentropy([0,1,0,0],[0.25,0.25,0.25,0.25]))\n",
    "# 预测错了，交叉熵2.3\n",
    "print(tf.losses.categorical_crossentropy([0,1,0,0],[0.1,0.1,0.7,0.1]))\n",
    "# 预测对了，交叉熵为0.35\n",
    "print(tf.losses.categorical_crossentropy([0,1,0,0],[0.1,0.7,0.1,0.1]))\n",
    "print('=================================================================================================================================')\n",
    "\n",
    "\"\"\"\n",
    "(2) 二分类问题交叉熵\n",
    "tf.losses.binary_crossentropy()\n",
    "参数参考：https://blog.csdn.net/weixin_46649052/article/details/112707572\n",
    "\"\"\"\n",
    "# 二分类\n",
    "# 预测对了，0.1\n",
    "print(tf.losses.binary_crossentropy([1,0],[0.9,0.1]))\n",
    "# 预测错了，2.3\n",
    "print(tf.losses.categorical_crossentropy([1,0],[0.1,0.9]))\n",
    "print('=================================================================================================================================')\n",
    "\n",
    "\"\"\"\n",
    "(3) logits层直接计算交叉熵\n",
    "模型的输出结果可能不是概率形式，通过softmax函数转换为概率形式，然后计算交叉熵，但有时候会出现数据不稳定的情况，即输出结果是NAN或者inf。\n",
    "这种情况下可以通过logits层直接计算交叉熵，不过要给categorical_crossentropy()方法传递一个from_logits=True参数。\n",
    "\"\"\"\n",
    "# 计算网络损失时，一定要加数值稳定处理参数from_logits=True，防止softmax和crossentropy之间的数值不稳定\n",
    "import tensorflow as tf\n",
    "x = tf.random.normal([1,784]) # 1张图片784个特征\n",
    "w = tf.random.normal([784,2]) # 全连接层，分为2类\n",
    "b = tf.zeros([2]) # 2个偏置\n",
    " \n",
    "logits = x@w + b # 计算logits层\n",
    "print(logits)\n",
    "# 不推荐使用下面这种，会出现数值不稳定\n",
    "# 输出结果映射到0-1，且概率和为1，(这一步由from_logits=True代替)\n",
    "# prob = tf.math.softmax(logits,axis=1)\n",
    "# tf.losses.categorical_crossentropy([[0,1]],prob)\n",
    " \n",
    "# 计算交叉熵，一定要对y_true进行onehot编码\n",
    "print(tf.losses.categorical_crossentropy([[0,1]],logits,from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "528383ee-d462-4198-8c35-585250da12ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.32023937  0.29598814  0.02425124]\n",
      " [-1.3392388   0.28490427  1.0543346 ]\n",
      " [-0.5359759   0.03919774  0.49677816]\n",
      " [-0.40006688  0.26153514  0.13853176]], shape=(4, 3), dtype=float32)\n",
      "tf.Tensor([ 0.00100154  0.2935928  -0.29459435], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.4 交叉熵的梯度\n",
    "损失函数为计算交叉熵的平均值，一定要对训练集真实值y_true进行one-hot编码，分三类，tf.one_hot(y,depth=3)，\n",
    "跟踪权重w和偏置b，grads[0]为以loss为因变量，w为自变量计算梯度。grads[1]为以loss为因变量，b为自变量计算梯度\n",
    "\"\"\"\n",
    "#（2）交叉熵cross entropy\n",
    "# softmax，使logits数值最大的所在的所有作为预测结果的label\n",
    "x = tf.random.normal([2,4]) # 输入层，2张图片，4个特征\n",
    "w = tf.random.normal([4,3]) # 一层全连接层，输出三个分类\n",
    "b = tf.zeros([3]) # 三个偏置\n",
    "y = tf.constant([2,0]) # 第一个样本属于2类别，第2个样本属于0类别\n",
    "# 梯度计算\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([w,b]) # 跟踪w和b的梯度\n",
    "    logits = x @ w + b # 计算logits层\n",
    "    # 计算损失，不要先softmax再使用交叉熵，会导致数据不稳定，categorical_crossentropy会自动执行softmax操作再计算损失\n",
    "    loss = tf.reduce_mean(tf.losses.categorical_crossentropy(tf.one_hot(y,depth=3),logits,from_logits=True))\n",
    "# 计算损失函数梯度\n",
    "grads = tape.gradient(loss,[w,b])\n",
    "print(grads[0]) #shape为[4,3]\n",
    "print(grads[1]) #shape为[3]\n",
    " \n",
    " \n",
    "# 结果为grads[0]：\n",
    "# array([[ 0.05101332,  0.08121025, -0.13222364],\n",
    "#        [ 0.1871956 , -0.02524163, -0.16195397],\n",
    "#        [-1.6817021 ,  0.17055441,  1.5111479 ],\n",
    "#        [-0.08085182,  0.06344394,  0.01740783]], dtype=float32)>\n",
    "# grads[1]：\n",
    "# <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.12239906,  0.08427953,  0.03811949], dtype=float32)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6ebde-8e7d-4df2-be62-bd8e428a5cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
