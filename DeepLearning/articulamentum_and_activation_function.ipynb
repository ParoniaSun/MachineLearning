{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b81ce53-75a5-4686-9fd7-0bc52784d9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:52:09) \n",
      "[Clang 14.0.6 ]\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6ef94-63ad-44cc-a10d-cbdefce2392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(1) 全连接层创建： tf.keras.Sequential()，tf.keras.layers.Dense()\n",
    "(2) 输出方式： tf.sigmoid()，tf.softmax()，tf.tanh()，tf.nn.relu()，tf.nn.leaky_relu()\n",
    "\n",
    "1. 全连接层\n",
    "全连接层在整个网络卷积神经网络中起到特征提取器的作用。全连接层将学到的特征表示映射到样本的标记空间。\n",
    "\n",
    "（1）在全连接层中创建一层： tf.keras.layers.Dense()\n",
    "units： 正整数，输出空间的维数\n",
    "activation=None： 激活函数，不指定则没有\n",
    "use_bias=True： 布尔值，是否使用偏移向量\n",
    "kernel_initializer='glorot_uniform'： 核权重矩阵的初始值设定项\n",
    "bias_initializer='zeros'： 偏差向量的初始值设定项\n",
    "kernel_regularizer=None： 正则化函数应用于核权矩阵\n",
    "bias_regularizer=None： 应用于偏差向量的正则化函数\n",
    "kernel_constraint=None： 对权重矩阵约束\n",
    "bias_constraint=None： 对偏置向量约束\n",
    "\n",
    "（2）堆层构建全连接层： tf.keras.Sequential([layer1,layer2,layer3])\n",
    "通过组合层来构建模型，直接堆叠各个层\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca650a54-f15a-4eeb-81ff-cb14f6eac552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.1 创建单个连接层\n",
    "首先构造一个shape为[4,478]的随机正态分布，构造一个连接层tf.keras.layers.Dense()，输出512个特征，shape为[4,512]。\n",
    "将输入层放入连接层自动调用build()函数创建权重w和偏置b，将计算结果返回给out。\n",
    "\"\"\"\n",
    "# 创建单个连接层\n",
    "import tensorflow as tf\n",
    "# 构建输入\n",
    "x = tf.random.normal([4,478])\n",
    "# 输入的shape不需要指定，可以自动的根据输入的特征生成对应的权值\n",
    "net = tf.keras.layers.Dense(512)\n",
    "# 输出结果\n",
    "out = net(x)\n",
    "out.shape  # 输出结果的shape[4,512]\n",
    "net.kernel.shape # 权值W的形状，W的shape为[478,512]\n",
    "net.bias.shape # 偏置b的形状，X的shape为[4,478]\n",
    "# 调用net函数时，自动判断，如果当前没有创建w和b，自动调用build函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfbb94e1-992e-4254-8376-4ca8fafe47eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 16)                176       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 984\n",
      "Trainable params: 984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "dense_1/kernel:0 (10, 16)\n",
      "dense_1/bias:0 (16,)\n",
      "dense_2/kernel:0 (16, 32)\n",
      "dense_2/bias:0 (32,)\n",
      "dense_3/kernel:0 (32, 8)\n",
      "dense_3/bias:0 (8,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.2 创建多个连接层\n",
    "使用tf.keras.Sequential()堆叠构建全连接层；使用tf.keras.layers.Dense()创建每一个全连接层；使用build()构建输入层。\n",
    "网络结构为[b,10]=>[b,16]=>[b,32]=>[b,8]；通过summary()查看整体网络架构。通过model.trainable_variables查看网络的所有的权重和偏置。\n",
    "\n",
    "整体网络构架如下：对于第一层，w的shape为[10,16]，b的shape为[16]，共10*16+16=176个参数 ，下面几层的参数个数同理\n",
    "\"\"\"\n",
    "#（4）创建多个全连接层\n",
    "# 将所有连接层组成一个列表放入\n",
    "from tensorflow import keras\n",
    "# 构建全连接层\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation='relu'), #w的shape为[10,16]，b的shape为[16]，共10*16+16=176个参数  \n",
    "    keras.layers.Dense(32, activation='relu'), #w的shape为[16,32]，b的shape为[32]\n",
    "    keras.layers.Dense(8)])  #w的shape为[32,8]，b的shape为[8]\n",
    " \n",
    "model.build(input_shape=[None,10]) #输入层10个特征\n",
    "model.summary()  # 查看网络结构\n",
    "# 查看网络所有的权重w和偏置b\n",
    "for p in model.trainable_variables:\n",
    "    print(p.name,p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d69bde3d-48a2-4656-9af5-9e7438fa17aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-6.         -4.66666667 -3.33333333 -2.         -0.66666667  0.66666667\n",
      "  2.          3.33333333  4.66666667  6.        ], shape=(10,), dtype=float64)\n",
      "=================================================================================================================================\n",
      "tf.Tensor(\n",
      "[0.00247262 0.00931596 0.0344452  0.11920292 0.33924363 0.66075637\n",
      " 0.88079708 0.9655548  0.99068404 0.99752738], shape=(10,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. 输出映射方式\n",
    "2.1 sigmoid函数\n",
    "sigmoid函数： tf.sigmoid(tensor)\n",
    "把输出结果的范围映射到(0,1)之间，计算公式：\\frac{1}{1+e^{-x}}\n",
    "\n",
    "sigmoid只能保证单个数值的范围在0-1之间\n",
    "\"\"\"\n",
    "a = tf.linspace(-6,6,10) #-6到6的长度为10的向量\n",
    "print(a)\n",
    "print('=================================================================================================================================')\n",
    "# sigmoid函数将其映射到0-1\n",
    "print(tf.sigmoid(a))\n",
    "# 输入数据：array([-6.        , -4.66666667, -3.33333333, -2.        , -0.66666667,0.66666667,  2.        ,  3.33333333,  4.66666667,  6.   \n",
    "# 输出结果：array([0.00247262, 0.00931596, 0.0344452 , 0.11920292, 0.33924363,0.66075637, 0.88079708, 0.9655548 , 0.99068404, 0.99752738])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87aa1df0-c6ae-480b-9d64-1c9491f7e761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.788518   -0.10648251 -1.6808982  -1.3044801  -1.042417    0.53260565\n",
      "  -0.28296232  0.30348158 -0.09753752 -0.51368237]], shape=(1, 10), dtype=float32)\n",
      "=================================================================================================================================\n",
      "tf.Tensor(\n",
      "[[0.45981377 0.06911841 0.01431639 0.02085979 0.02710956 0.13096227\n",
      "  0.05793614 0.10414511 0.06973945 0.04599907]], shape=(1, 10), dtype=float32)\n",
      "=================================================================================================================================\n",
      "tf.Tensor([1.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.2 softmax函数\n",
    "对于分类问题，需要每一个概率在0-1之间，且总的概率和为1。\n",
    "softmax函数：tf.softmax(logits, axis)\n",
    "logits：一个非空的Tensor。必须是下列类型之一：half， float32，float64。\n",
    "axis：将在其上执行维度softmax。默认值为-1，表示最后一个维度。\n",
    "公式为：\\sigma (z)_{j}=\\frac{e^zj}{\\sum_{k=1}^{K}e^{z}k} \\;\\;\\;\\; for\\;\\;j=1,...,K\n",
    "\n",
    "一般将没有加激活函数的称为Logits，加了softmax后称为Probabilities，经过softmax后，有把最大值放大的过程，相当于把强的变得更强，把弱的变得更弱。\n",
    "\"\"\"\n",
    "logits = tf.random.uniform([1,10],minval=-2,maxval=2)\n",
    "print(logits)\n",
    "print('=================================================================================================================================')\n",
    "\n",
    "# 使用softmax\n",
    "prob = tf.nn.softmax(logits,axis=1) #对整个列表的值求softmax\n",
    "print(prob)\n",
    "print('=================================================================================================================================')\n",
    "\n",
    "# 求和\n",
    "print(tf.reduce_sum(prob,axis=1))\n",
    " \n",
    "# logits：array([[-0.80400324,  1.1189251 ,  1.7390656 ,  0.8317995 , -1.1970901 ,-0.48806524, -1.1161256 , -0.35890388,  0.765254  , -1.8393846 ]],dtype=float32)>\n",
    "# prob：array([[0.02841684, 0.19439851, 0.3614236 , 0.14588003, 0.01918051,0.03897498, 0.02079806, 0.0443486 , 0.13648833, 0.01009056]],dtype=float32)>\n",
    "# sum：<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f8c6e0a-c569-469f-9a55-0d88a59e056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-2. -1.  0.  1.  2.], shape=(5,), dtype=float64)\n",
      "=================================================================================================================================\n",
      "tf.Tensor([-0.96402758 -0.76159416  0.          0.76159416  0.96402758], shape=(5,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.3 tanh函数\n",
    "tanh是通过sigmoid函数放大平移得到，输出结果在(-1,1)之间\n",
    "\n",
    "tanh函数： tf.tanh(tensor)\n",
    "\"\"\"\n",
    "a = tf.linspace(-2, 2, 5)  # -2到2的长度为5的向量\n",
    "print(a)\n",
    "print('=================================================================================================================================')\n",
    "\n",
    "# 结果映射到(-1,1)\n",
    "print(tf.tanh(a))\n",
    "# 输入：<tf.Tensor: shape=(5,), dtype=float64, numpy=array([-2., -1.,  0.,  1.,  2.])>\n",
    "# 输出：<tf.Tensor: shape=(5,), dtype=float64, numpy=array([-0.96402758, -0.76159416,  0.        ,  0.76159416,  0.96402758])>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf7378f8-aedf-4d4d-ad1a-2c8040be474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-1.         -0.77777778 -0.55555556 -0.33333333 -0.11111111  0.11111111\n",
      "  0.33333333  0.55555556  0.77777778  1.        ], shape=(10,), dtype=float64)\n",
      "=================================================================================================================================\n",
      "tf.Tensor(\n",
      "[0.         0.         0.         0.         0.         0.11111111\n",
      " 0.33333333 0.55555556 0.77777778 1.        ], shape=(10,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.4 relu函数\n",
    "将输入小于0的值都变成0，大于0的值保持不变。\n",
    "\n",
    "relu激活函数：tf.nn.relu(features)\n",
    "公式为： f(x)=\\begin{bmatrix} 0\\;\\,\\;x<0 & \\\\ x \\;\\;\\;x>=0 & \\end{bmatrix}\n",
    "\"\"\"\n",
    "# relu函数，x小于0，梯度0，x大于0梯度1\n",
    "a = tf.linspace(-1,1,10) # -1到1，长度为10的向量\n",
    "print(a)\n",
    "print('=================================================================================================================================')\n",
    "\n",
    "print(tf.nn.relu(a))\n",
    " \n",
    "# 输入为：array([-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,        0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ])>\n",
    "# 输出为：array([0.        , 0.        , 0.        , 0.        , 0.        ,       0.11111111, 0.33333333, 0.55555556, 0.77777778, 1.        ])>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b3ff706-6b5c-4b06-83c3-a1cbd20ce2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-1.         -0.77777778 -0.55555556 -0.33333333 -0.11111111  0.11111111\n",
      "  0.33333333  0.55555556  0.77777778  1.        ], shape=(10,), dtype=float64)\n",
      "=================================================================================================================================\n",
      "tf.Tensor(\n",
      "[-0.2        -0.15555556 -0.11111111 -0.06666667 -0.02222222  0.11111111\n",
      "  0.33333333  0.55555556  0.77777778  1.        ], shape=(10,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.5 leaky_relu函数\n",
    "非饱和激活函数： tf.nn.leaky_relu( features, alpha=0.2 )\n",
    "features： tensor类型，表示预激活值，必须是下列类型之一：float16,float32,float64,int32,int64.\n",
    "alpha： x <0时激活函数的斜率.\n",
    "由于relu函数对于小于0的值，这个神经元的梯度永远都会是0，在实际操作中，如果数据很大，很可能网络中较多的神经元都死了。这时需要leaky_relu函数：\n",
    "公式：y = max(0, x) + leak*min(0,x) \n",
    "leak是一个很小的常数，这样保留了一些负轴的值，使得负轴的信息不会全部丢失\n",
    "\"\"\"\n",
    "a = tf.linspace(-1,1,10)\n",
    "print(a)\n",
    "print('=================================================================================================================================')\n",
    "\n",
    "# tf.nn.leaky_rulu() ，x小于0y也小于0但接近0\n",
    "print(tf.nn.leaky_relu(a))\n",
    " \n",
    "# 输入：array([-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,        0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ])>\n",
    "# 输出：array([-0.2       , -0.15555556, -0.11111111, -0.06666667, -0.02222222,        0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ])>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
